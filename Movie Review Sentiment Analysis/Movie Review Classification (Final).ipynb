{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('IMDB_Dataset.csv')             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning dataset for Feature Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []                                                      #Creating An empty list.\n",
    "for i in range(0,20000):                                            #Using only 20k reviews because it take lot of time.\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['review'][i])      #Keeping Alphabets and removing punctuation  \n",
    "    review = review.lower()                                      #converting to lower case\n",
    "    review = review.split()                                      #tokenizing\n",
    "    ps = PorterStemmer()                                         #stemming-reducing inflected words to their word stem\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)                                    #Removing unnecessary words and Join words to form sentence.\n",
    "    corpus.append(review)                                        #Placing all the cleaned review into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5000)        #Extraction Features from the Cleaned review to classify its as postive or negative.                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify training and test data from the featureset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus).toarray()                                        #X dataset       \n",
    "\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "y=y[:20000]                                                                      #cosidering output of only 20k reviews from 10k\n",
    "\n",
    "le = preprocessing.LabelEncoder()                                             # Converting catagorical data \n",
    "\n",
    "y = le.fit_transform(y)                                                       # y set  \n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)   #spliting X and y into train and test data wih ratio of 80:20    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train a Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(probability=True)                                 #using svm to classify reviews\n",
    "classifier.fit(X_train, y_train)                   #fitting the X_train, y_train to the model and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)                                        #useing unseen data to test the model\n",
    "\n",
    "# verifying  the model performance for new reviews.\n",
    "reviews = [\"I love this movie\",\"This movie is bad\",\"I was going to say something awesome or great or good, but I simply can't because the movie is so bad.\",\"It might have bad actors, but everything else is good.\",\"This movie turned out to be better than I had expected it to be. Some parts were pretty funny. It was nice to have a movie with a new plot.\",\"First one was much better, I had enjoyed it a lot. This one has not even produced a smile. The idea was showing how deep down can human kind fall, but in reference to the characters not the film-maker.\"]\n",
    "for review in reviews:\n",
    "    op=classifier.predict(cv.transform([review]).toarray())\n",
    "    if op==[0] :\n",
    "        print(review,'=','negative')\n",
    "    else :\n",
    "        print(review,'=','positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM-Evaluation:\\n')                                 \n",
    "print('Accuracy score : ',accuracy_score(y_test, y_pred)*100,'\\n')      #accuracy of the model     \n",
    "print(confusion_matrix(y_test,y_pred),'\\n')                             #Confusion Matrix\n",
    "print(classification_report(y_test,y_pred))                             #Classification Report such recall precision etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_probs = classifier.predict_proba(X_test)\n",
    "                                                                           # keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "                                                                           # predict class values\n",
    "yhat = classifier.predict(X_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "                                                                            # plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='svm')\n",
    "                                                                               # axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "                                                                                 # show the legend\n",
    "pyplot.legend()\n",
    "                                                                                 # show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
